{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zioGJQ7dn5IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d39c61-0d51-4100-f12d-cf7c0c2b4f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.6\n",
            "2.5.0+cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.2%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cu117\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu117.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_sparse-0.6.18%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cu117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install matplotlib\n",
        "!pip install rdkit\n",
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.1+cu117.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.1+cu117.html\n",
        "\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_embedding(h, color, epoch=None, loss=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    h = h.detach().cpu().numpy()\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    if epoch is not None and loss is not None:\n",
        "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "from torch_geometric.nn import BatchNorm, PNAConv, global_add_pool\n",
        "from torch_geometric.utils import degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3cDlq58gWmFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dbd1ea-abe3-4c73-bea9-f71bc6664800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of SMILES in QM9 dataset: 133885\n",
            "Example SMILES: N\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def edge_index_to_adj(edge_index, num_nodes, edge_attr=None):\n",
        "    \"\"\"\n",
        "    Convert edge index to an adjacency matrix with optional edge attributes.\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): Edge indices of shape [2, num_edges].\n",
        "        num_nodes (int): The number of nodes in the graph.\n",
        "        edge_attr (torch.Tensor, optional): Edge attributes of shape [num_edges],\n",
        "                                            where each entry represents a bond type\n",
        "                                            (e.g., 1 for single, 2 for double).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: An adjacency matrix of shape [num_nodes, num_nodes] where\n",
        "                      each entry reflects the type of edge (bond type) between nodes.\n",
        "    \"\"\"\n",
        "    # Create an empty adjacency matrix\n",
        "    adj = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
        "\n",
        "    # If no edge attributes are provided, default to 1 for all edges\n",
        "    if edge_attr is None:\n",
        "        edge_attr = torch.ones(edge_index.size(1), dtype=torch.float)\n",
        "    else:\n",
        "        edge_attr = edge_attr.to(torch.float)  # Ensure edge_attr is of the same type as adj\n",
        "\n",
        "    # Fill the adjacency matrix using the edge indices and edge attributes\n",
        "    adj[edge_index[0], edge_index[1]] = edge_attr\n",
        "    adj[edge_index[1], edge_index[0]] = edge_attr  # For undirected graphs\n",
        "\n",
        "    return adj\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "\n",
        "def load_qm9_smiles(csv_file):\n",
        "    # Read the CSV file containing the QM9 dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extract SMILES strings\n",
        "    smiles_list = df['smiles'].tolist()\n",
        "\n",
        "    return smiles_list\n",
        "\n",
        "# Example usage\n",
        "csv_file = \"qm9.csv\"  # Replace with the path to your QM9 CSV file\n",
        "qm9_smiles = load_qm9_smiles(csv_file)\n",
        "\n",
        "print(\"Number of SMILES in QM9 dataset:\", len(qm9_smiles))\n",
        "print(\"Example SMILES:\", qm9_smiles[1])\n",
        "def remove_hydrogen_from_smiles(smiles_list):\n",
        "    modified_smiles = []\n",
        "    for smiles in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            print(\"Invalid SMILES:\", smiles)\n",
        "            continue\n",
        "        mol = Chem.RemoveHs(mol)\n",
        "        modified_smiles.append(Chem.MolToSmiles(mol))\n",
        "    return modified_smiles\n",
        "\n",
        "# Example usage\n",
        "# Assuming qm9_smiles is a list containing SMILES strings from the QM9 dataset\n",
        "modified_smiles = remove_hydrogen_from_smiles(qm9_smiles)\n",
        "\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    # Parse the SMILES string\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Get node features (atomic numbers)\n",
        "    atomic_numbers = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "\n",
        "    # Get edge indices and edge types (bond types)\n",
        "    edge_index = []\n",
        "    edge_types = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start_idx = bond.GetBeginAtomIdx()\n",
        "        end_idx = bond.GetEndAtomIdx()\n",
        "        edge_index.append([start_idx, end_idx])\n",
        "        edge_index.append([end_idx, start_idx])  # Include both directions for undirected graph\n",
        "\n",
        "        # Encode bond type as an integer\n",
        "        bond_type = bond.GetBondType()\n",
        "        if bond_type == Chem.rdchem.BondType.SINGLE:\n",
        "            edge_types.append(1)\n",
        "            edge_types.append(1)  # Add both directions\n",
        "        elif bond_type == Chem.rdchem.BondType.DOUBLE:\n",
        "            edge_types.append(2)\n",
        "            edge_types.append(2)\n",
        "        elif bond_type == Chem.rdchem.BondType.TRIPLE:\n",
        "            edge_types.append(3)\n",
        "            edge_types.append(3)\n",
        "        elif bond_type == Chem.rdchem.BondType.AROMATIC:\n",
        "            edge_types.append(4)\n",
        "            edge_types.append(4)\n",
        "        else:\n",
        "            edge_types.append(0)  # Unknown bond type\n",
        "            edge_types.append(0)\n",
        "\n",
        "    # Convert edge indices and edge types to PyTorch tensors\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Shape [2, num_edges]\n",
        "    edge_types = torch.tensor(edge_types, dtype=torch.long)  # Shape [num_edges]\n",
        "\n",
        "    # Convert node features to PyTorch tensor\n",
        "    node_features = torch.tensor(atomic_numbers, dtype=torch.float).unsqueeze(1)  # Shape [num_nodes, 1]\n",
        "\n",
        "    return node_features, edge_index, edge_types\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "filtered_dataset = []\n",
        "\n",
        "# Define encoding mappings for atomic numbers to one-hot encodings\n",
        "encoding_mappings = {\n",
        "    7: [0, 0, 1, 0, 0],  # Nitrogen\n",
        "    8: [0, 0, 0, 1, 0],  # Oxygen\n",
        "    6: [0, 1, 0, 0, 0],  # Carbon\n",
        "    9: [0, 0, 0, 0, 1]   # Fluorine\n",
        "}\n",
        "\n",
        "# Iterate over modified SMILES\n",
        "for smile in modified_smiles:\n",
        "    try:\n",
        "        # Convert SMILES to graph representation with node features, edge index, and edge types\n",
        "        node_features, edge_index1, edge_types = smiles_to_graph(smile)\n",
        "\n",
        "        # Check if the graph has more than one node\n",
        "        num_nodes = node_features.shape[0]\n",
        "        if num_nodes > 1:\n",
        "            # Convert node features to one-hot encoding\n",
        "            one_hot_encoded = torch.tensor([encoding_mappings[num.item()] for num in node_features.squeeze()], dtype=torch.float32)\n",
        "\n",
        "\n",
        "            # Create Data object with x, edge_index, and edge_attr for edge types\n",
        "            graph = Data(x=one_hot_encoded, edge_index=edge_index1, edge_attr=edge_types, num_nodes=num_nodes)\n",
        "            filtered_dataset.append(graph)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing SMILES: {smile}. {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining GNN related code\n"
      ],
      "metadata": {
        "id": "CnxeYDgbKsQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c8bc1a81-5b99-4b6e-8091-a9d44f3142d3"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def unsorted_segment_sum(data, segment_ids, num_segments, normalization_factor, aggregation_method: str):\n",
        "    \"\"\"Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n",
        "        Normalization: 'sum' or 'mean'.\n",
        "    \"\"\"\n",
        "    result_shape = (num_segments, data.size(1))\n",
        "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
        "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
        "    result.scatter_add_(0, segment_ids, data)\n",
        "    if aggregation_method == 'sum':\n",
        "        result = result / normalization_factor\n",
        "\n",
        "    if aggregation_method == 'mean':\n",
        "        norm = data.new_zeros(result.shape)\n",
        "        norm.scatter_add_(0, segment_ids, data.new_ones(data.shape))\n",
        "        norm[norm == 0] = 1\n",
        "        result = result / norm\n",
        "    return result\n",
        "\n",
        "class GCL(nn.Module):\n",
        "    def __init__(self, input_nf, output_nf, hidden_nf, normalization_factor, aggregation_method,\n",
        "                 edges_in_d=0, nodes_att_dim=0, act_fn=nn.SiLU(), attention=False):\n",
        "        super(GCL, self).__init__()\n",
        "        input_edge = input_nf * 2\n",
        "        self.normalization_factor = normalization_factor\n",
        "        self.aggregation_method = aggregation_method\n",
        "        self.attention = attention\n",
        "\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(input_edge + edges_in_d, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, hidden_nf),\n",
        "            act_fn)\n",
        "\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_nf + input_nf + nodes_att_dim, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, output_nf))\n",
        "\n",
        "        if self.attention:\n",
        "            self.att_mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_nf, 1),\n",
        "                nn.Sigmoid())\n",
        "\n",
        "    def edge_model(self, source, target, edge_attr, edge_mask):\n",
        "        if edge_attr is None:  # Unused.\n",
        "            out = torch.cat([source, target], dim=1)\n",
        "        else:\n",
        "            out = torch.cat([source, target, edge_attr], dim=1)\n",
        "\n",
        "        mij = self.edge_mlp(out)\n",
        "\n",
        "        if self.attention:\n",
        "            att_val = self.att_mlp(mij)\n",
        "            out = mij * att_val\n",
        "        else:\n",
        "            out = mij\n",
        "\n",
        "        if edge_mask is not None:\n",
        "            out = out * edge_mask\n",
        "        return out, mij\n",
        "\n",
        "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
        "        row, col = edge_index\n",
        "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0),\n",
        "                                   normalization_factor=self.normalization_factor,\n",
        "                                   aggregation_method=self.aggregation_method)\n",
        "        if node_attr is not None:\n",
        "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
        "        else:\n",
        "            agg = torch.cat([x, agg], dim=1)\n",
        "        out = x + self.node_mlp(agg)\n",
        "        return out, agg\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr=None, node_attr=None, node_mask=None, edge_mask=None):\n",
        "        row, col = edge_index\n",
        "        edge_feat, mij = self.edge_model(h[row], h[col], edge_attr, edge_mask)\n",
        "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
        "        if node_mask is not None:\n",
        "            h = h * node_mask\n",
        "        return h, mij\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, in_node_nf, in_edge_nf, hidden_nf, out_node_nf,aggregation_method='sum', device='cpu',\n",
        "                 act_fn=nn.SiLU(), n_layers=4, attention=False,\n",
        "                 normalization_factor=100, ):\n",
        "        super(GNN, self).__init__()\n",
        "        if out_node_nf is None:\n",
        "            out_node_nf = in_node_nf\n",
        "        self.hidden_nf = hidden_nf\n",
        "        self.device = device\n",
        "        self.n_layers = n_layers\n",
        "        ### Encoder\n",
        "        self.embedding = nn.Linear(in_node_nf, self.hidden_nf)\n",
        "        self.embedding_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
        "        for i in range(0, n_layers):\n",
        "            self.add_module(\"gcl_%d\" % i, GCL(\n",
        "                self.hidden_nf, self.hidden_nf, self.hidden_nf,\n",
        "                normalization_factor=normalization_factor,\n",
        "                aggregation_method=aggregation_method,\n",
        "                edges_in_d=in_edge_nf, act_fn=act_fn,\n",
        "                attention=attention))\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, h, edges, edge_attr=None, node_mask=None, edge_mask=None):\n",
        "        # Edit Emiel: Remove velocity as input\n",
        "        h = self.embedding(h)\n",
        "        for i in range(0, self.n_layers):\n",
        "            h, _ = self._modules[\"gcl_%d\" % i](h, edges, edge_attr=edge_attr, node_mask=node_mask, edge_mask=edge_mask)\n",
        "        h = self.embedding_out(h)\n",
        "\n",
        "        # Important, the bias of the last linear might be non-zero\n",
        "        if node_mask is not None:\n",
        "            h = h * node_mask\n",
        "        return h\n",
        "import numpy as np\n",
        "\n",
        "def fully_connected_graph(num_nodes):\n",
        "    # Generate all possible pairs of nodes\n",
        "    nodes = np.arange(num_nodes)\n",
        "    pairs = np.array(np.meshgrid(nodes, nodes)).T.reshape(-1, 2)\n",
        "\n",
        "    # Filter out self-loops (optional, depending on your requirements)\n",
        "    pairs = pairs[pairs[:, 0] != pairs[:, 1]]\n",
        "\n",
        "    # Create the edge index tensor\n",
        "    edge_index = torch.tensor(pairs, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "# Example usage for a fully connected graph with 4 nodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining EGNN related code"
      ],
      "metadata": {
        "id": "TekKAUyCKzkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "06eaf32b-0b98-4070-89ba-45a1b13ea85d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def coord2diff(x, edge_index, norm_constant=1):\n",
        "    row, col = edge_index\n",
        "    coord_diff = x[row] - x[col]\n",
        "    radial = torch.sum((coord_diff) ** 2, 1).unsqueeze(1)\n",
        "    norm = torch.sqrt(radial + 1e-8)\n",
        "    coord_diff = coord_diff/(norm + norm_constant)\n",
        "    return radial, coord_diff\n",
        "class GCL(nn.Module):\n",
        "    def __init__(self, input_nf, output_nf, hidden_nf, normalization_factor, aggregation_method,\n",
        "                 edges_in_d=0, nodes_att_dim=0, act_fn=nn.SiLU(), attention=False):\n",
        "        super(GCL, self).__init__()\n",
        "        input_edge = input_nf * 2\n",
        "        self.normalization_factor = normalization_factor\n",
        "        self.aggregation_method = aggregation_method\n",
        "        self.attention = attention\n",
        "\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(input_edge + edges_in_d, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, hidden_nf),\n",
        "            act_fn)\n",
        "\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_nf + input_nf + nodes_att_dim, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, output_nf))\n",
        "\n",
        "        if self.attention:\n",
        "            self.att_mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_nf, 1),\n",
        "                nn.Sigmoid())\n",
        "\n",
        "    def edge_model(self, source, target, edge_attr, edge_mask):\n",
        "        if edge_attr is None:  # Unused.\n",
        "            out = torch.cat([source, target], dim=1)\n",
        "        else:\n",
        "            out = torch.cat([source, target, edge_attr], dim=1)\n",
        "        mij = self.edge_mlp(out)\n",
        "\n",
        "        if self.attention:\n",
        "            att_val = self.att_mlp(mij)\n",
        "            out = mij * att_val\n",
        "        else:\n",
        "            out = mij\n",
        "\n",
        "        if edge_mask is not None:\n",
        "            out = out * edge_mask\n",
        "        return out, mij\n",
        "\n",
        "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
        "        row, col = edge_index\n",
        "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0),\n",
        "                                   normalization_factor=self.normalization_factor,\n",
        "                                   aggregation_method=self.aggregation_method)\n",
        "        if node_attr is not None:\n",
        "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
        "        else:\n",
        "            agg = torch.cat([x, agg], dim=1)\n",
        "        out = x + self.node_mlp(agg)\n",
        "        return out, agg\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr=None, node_attr=None, node_mask=None, edge_mask=None):\n",
        "        row, col = edge_index\n",
        "        edge_feat, mij = self.edge_model(h[row], h[col], edge_attr, edge_mask)\n",
        "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
        "        if node_mask is not None:\n",
        "            h = h * node_mask\n",
        "        return h, mij\n",
        "\n",
        "\n",
        "class EquivariantUpdate(nn.Module):\n",
        "    def __init__(self, hidden_nf, normalization_factor, aggregation_method,\n",
        "                 edges_in_d=1, act_fn=nn.SiLU(), tanh=False, coords_range=10.0):\n",
        "        super(EquivariantUpdate, self).__init__()\n",
        "        self.tanh = tanh\n",
        "        self.coords_range = coords_range\n",
        "        input_edge = hidden_nf * 2 + edges_in_d\n",
        "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
        "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
        "        self.coord_mlp = nn.Sequential(\n",
        "            nn.Linear(input_edge, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, hidden_nf),\n",
        "            act_fn,\n",
        "            layer)\n",
        "        self.normalization_factor = normalization_factor\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "    def coord_model(self, h, coord, edge_index, coord_diff, edge_attr, edge_mask):\n",
        "        row, col = edge_index\n",
        "        input_tensor = torch.cat([h[row], h[col], edge_attr], dim=1)\n",
        "        if self.tanh:\n",
        "            trans = coord_diff * torch.tanh(self.coord_mlp(input_tensor)) * self.coords_range\n",
        "        else:\n",
        "            trans = coord_diff * self.coord_mlp(input_tensor)\n",
        "        if edge_mask is not None:\n",
        "            trans = trans * edge_mask\n",
        "        agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0),\n",
        "                                   normalization_factor=self.normalization_factor,\n",
        "                                   aggregation_method=self.aggregation_method)\n",
        "        coord = coord + agg\n",
        "        return coord\n",
        "\n",
        "    def forward(self, h, coord, edge_index, coord_diff, edge_attr=None, node_mask=None, edge_mask=None):\n",
        "        coord = self.coord_model(h, coord, edge_index, coord_diff, edge_attr, edge_mask)\n",
        "        if node_mask is not None:\n",
        "            coord = coord * node_mask\n",
        "        return coord\n",
        "\n",
        "\n",
        "class EquivariantBlock(nn.Module):\n",
        "    def __init__(self, hidden_nf, edge_feat_nf=2, device='cpu', act_fn=nn.SiLU(), n_layers=2, attention=True,\n",
        "                 norm_diff=True, tanh=False, coords_range=15, norm_constant=1, sin_embedding=None,\n",
        "                 normalization_factor=100, aggregation_method='sum'):\n",
        "        super(EquivariantBlock, self).__init__()\n",
        "        self.hidden_nf = hidden_nf\n",
        "        self.device = device\n",
        "        self.n_layers = n_layers\n",
        "        self.coords_range_layer = float(coords_range)\n",
        "        self.norm_diff = norm_diff\n",
        "        self.norm_constant = norm_constant\n",
        "        self.sin_embedding = sin_embedding\n",
        "        self.normalization_factor = normalization_factor\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "        for i in range(0, n_layers):\n",
        "            self.add_module(\"gcl_%d\" % i, GCL(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=edge_feat_nf,\n",
        "                                              act_fn=act_fn, attention=attention,\n",
        "                                              normalization_factor=self.normalization_factor,\n",
        "                                              aggregation_method=self.aggregation_method))\n",
        "        self.add_module(\"gcl_equiv\", EquivariantUpdate(hidden_nf, edges_in_d=edge_feat_nf, act_fn=nn.SiLU(), tanh=tanh,\n",
        "                                                       coords_range=self.coords_range_layer,\n",
        "                                                       normalization_factor=self.normalization_factor,\n",
        "                                                       aggregation_method=self.aggregation_method))\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, h, x, edge_index, node_mask=None, edge_mask=None, edge_attr=None):\n",
        "        # Edit Emiel: Remove velocity as input\n",
        "        distances, coord_diff = coord2diff(x, edge_index, self.norm_constant)\n",
        "        if self.sin_embedding is not None:\n",
        "            distances = self.sin_embedding(distances)\n",
        "        edge_attr = torch.cat([distances, edge_attr], dim=1)\n",
        "        for i in range(0, self.n_layers):\n",
        "            h, _ = self._modules[\"gcl_%d\" % i](h, edge_index, edge_attr=edge_attr, node_mask=node_mask, edge_mask=edge_mask)\n",
        "        x = self._modules[\"gcl_equiv\"](h, x, edge_index, coord_diff, edge_attr, node_mask, edge_mask)\n",
        "\n",
        "        # Important, the bias of the last linear might be non-zero\n",
        "        if node_mask is not None:\n",
        "            h = h * node_mask\n",
        "        return h, x\n",
        "\n",
        "\n",
        "class EGNN(nn.Module):\n",
        "    def __init__(self, in_node_nf, in_edge_nf, hidden_nf, device='cpu', act_fn=nn.SiLU(), n_layers=3, attention=False,\n",
        "                 norm_diff=True, out_node_nf=None, tanh=False, coords_range=15, norm_constant=1, inv_sublayers=2,\n",
        "                 sin_embedding=False, normalization_factor=100, aggregation_method='sum'):\n",
        "        super(EGNN, self).__init__()\n",
        "        if out_node_nf is None:\n",
        "            out_node_nf = in_node_nf\n",
        "        self.hidden_nf = hidden_nf\n",
        "        self.device = device\n",
        "        self.n_layers = n_layers\n",
        "        self.coords_range_layer = float(coords_range/n_layers) if n_layers > 0 else float(coords_range)\n",
        "        self.norm_diff = norm_diff\n",
        "        self.normalization_factor = normalization_factor\n",
        "        self.aggregation_method = aggregation_method\n",
        "\n",
        "        if sin_embedding:\n",
        "            self.sin_embedding = SinusoidsEmbeddingNew()\n",
        "            edge_feat_nf = self.sin_embedding.dim * 2\n",
        "        else:\n",
        "            self.sin_embedding = None\n",
        "            edge_feat_nf = 2\n",
        "\n",
        "        self.embedding = nn.Linear(in_node_nf, self.hidden_nf)\n",
        "        self.embedding_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
        "        for i in range(0, n_layers):\n",
        "            self.add_module(\"e_block_%d\" % i, EquivariantBlock(hidden_nf, edge_feat_nf=edge_feat_nf, device=device,\n",
        "                                                               act_fn=act_fn, n_layers=inv_sublayers,\n",
        "                                                               attention=attention, norm_diff=norm_diff, tanh=tanh,\n",
        "                                                               coords_range=coords_range, norm_constant=norm_constant,\n",
        "                                                               sin_embedding=self.sin_embedding,\n",
        "                                                               normalization_factor=self.normalization_factor,\n",
        "                                                               aggregation_method=self.aggregation_method))\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, h, x, edge_index, node_mask=None, edge_mask=None):\n",
        "        # Edit Emiel: Remove velocity as input\n",
        "        distances, _ = coord2diff(x, edge_index)\n",
        "        if self.sin_embedding is not None:\n",
        "            distances = self.sin_embedding(distances)\n",
        "        h = self.embedding(h)\n",
        "        for i in range(0, self.n_layers):\n",
        "            h, x = self._modules[\"e_block_%d\" % i](h, x, edge_index, node_mask=node_mask, edge_mask=edge_mask, edge_attr=distances)\n",
        "\n",
        "        # Important, the bias of the last linear might be non-zero\n",
        "        h = self.embedding_out(h)\n",
        "        if node_mask is not None:\n",
        "            h = h * node_mask\n",
        "        return h, x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c369cUY51Fag"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def fully_connected_graph_with_self_loops(num_nodes):\n",
        "    \"\"\"\n",
        "    Generates edge indices for a fully connected graph with self-loops.\n",
        "\n",
        "    Args:\n",
        "        num_nodes (int): Number of nodes in the graph.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Edge indices of the fully connected graph with self-loops.\n",
        "    \"\"\"\n",
        "    # Create edge indices for a fully connected graph with self-loops\n",
        "    edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes)])\n",
        "\n",
        "    return edge_index.t().contiguous()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the edge type prediction model"
      ],
      "metadata": {
        "id": "2adyqXNFK7Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, GATConv, MessagePassing\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class BondTypePredictor(nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize the BondTypePredictor model.\n",
        "\n",
        "        Args:\n",
        "            num_node_features (int): Number of input features for each node.\n",
        "            hidden_dim (int): Dimension of hidden layers in GNN.\n",
        "            num_classes (int): Number of bond types to predict.\n",
        "        \"\"\"\n",
        "        super(BondTypePredictor, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "        self.edge_classifier = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Node feature matrix of shape [num_nodes, num_node_features].\n",
        "            edge_index (torch.Tensor): Edge index matrix of shape [2, num_edges].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted bond types for each edge of shape [num_edges, num_classes].\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "\n",
        "        row, col = edge_index\n",
        "        edge_representation = torch.cat([x[row], x[col]], dim=1)\n",
        "\n",
        "\n",
        "        bond_type_logits = self.edge_classifier(edge_representation)\n",
        "\n",
        "        return bond_type_logits\n"
      ],
      "metadata": {
        "id": "hTrsLBdayxAD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "# Parameters\n",
        "node_feature_dim = 5\n",
        "hidden_dim = 32\n",
        "num_bond_types = 5\n",
        "\n",
        "# Initialize model\n",
        "model = BondTypePredictor(node_feature_dim, hidden_dim, num_bond_types)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "pOxa4BxB0ZHy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        bond_type_logits = model(data.x[:,:5], data.edge_index)\n",
        "\n",
        "\n",
        "        loss = criterion(bond_type_logits, data.edge_attr)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n"
      ],
      "metadata": {
        "id": "EMDPWTY40Q1-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training parameters\n",
        "num_epochs =20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model, filtered_dataset, criterion, optimizer)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "0GxRWKOezLyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# Convert SMILES to graph data\n",
        "def prepare_graph(smiles):\n",
        "    node_features, edge_index, edge_types = smiles_to_graph(smiles)\n",
        "    if node_features is None:\n",
        "        return None\n",
        "    return Data(x=node_features, edge_index=edge_index)\n"
      ],
      "metadata": {
        "id": "Bc0Vqw-KzL04"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(f'edge_type_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "YHheDL8SIqHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}